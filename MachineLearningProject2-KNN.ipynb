{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbe65057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "29badcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this class is merely for encapsulation.\n",
    "#this is how data of the dataset is stores in the objects \n",
    "#of this class.\n",
    "class DataSet:\n",
    "    #name of the dataset\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "    \n",
    "    def set_feature(self, features):\n",
    "        self.features = features\n",
    "    \n",
    "    def set_classes(self, classes):\n",
    "        self.classes = classes\n",
    "    \n",
    "    def get_classes(self):\n",
    "        return self.classes\n",
    "        \n",
    "    def set_org_data(self, org_data):\n",
    "        self.org_data = org_data\n",
    "        \n",
    "    def get_org_data(self):\n",
    "        return self.org_data\n",
    "    \n",
    "    # not writing all getters, setters, because\n",
    "    # they are not required in python (i didnt know that before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6c61a218",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class reads the data file and the names file\n",
    "#to load inti dataset object\n",
    "class Dataset_Reader:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.Path = 'C:\\\\Users\\\\nitas\\\\Downloads\\\\Machine Learning 2024\\\\MLProject2\\\\'\n",
    "    \n",
    "    #read the data from data file and fill the Datset class object\n",
    "    def read_data_file(self, ds):\n",
    "        if(ds.name == 'forestfires'):\n",
    "            df = pd.read_csv(self.Path+ds.name+'.data', delimiter=',')\n",
    "        else:    \n",
    "            df = pd.read_csv(self.Path+ds.name+'.data', delimiter=',', header=None)\n",
    "\n",
    "        ds.set_org_data(df)\n",
    "    \n",
    "    \n",
    "    #read name file, naming the features, just to fill\n",
    "    #for results, not mendatory function\n",
    "    def read_attributes(self, ds):\n",
    "        \n",
    "        start_string = 'Attribute Information'\n",
    "        stop_string = 'Missing Attribute Values'\n",
    "        isPrint = False\n",
    "        \n",
    "        #read section, where the file has information \n",
    "        #Attribute Information\n",
    "        with open(Path+ds.name+'.names', 'r') as file:\n",
    "            for line in file:\n",
    "                if start_string in line:\n",
    "                    isPrint = True\n",
    "                    #continue\n",
    "                    \n",
    "                if stop_string in line:\n",
    "                    isPrint = False\n",
    "                    break\n",
    "                        \n",
    "                if isPrint:\n",
    "                    print(line.strip())\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7733029a",
   "metadata": {},
   "source": [
    "# Pre Processing Class: Demonstration of Ten Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "12364246",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre Process Class, for data cleaning, \n",
    "#checking for any abnormalities, pre processes the data \n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "class PreProcess:\n",
    "    \n",
    "    def __init__(self, ds):\n",
    "        \n",
    "        #set other values of the Dataset object    \n",
    "        self.set_Dataset_features(ds)\n",
    "        \n",
    "        #min max scaling for the regression datasets\n",
    "        if(ds.type == 'Regression'):\n",
    "            self.min_max_scaling(ds)\n",
    "        #now first step is to do one-hot encoding\n",
    "        self.one_hot_encoding(ds)\n",
    "        #Now sort the data, if it is not sorted\n",
    "        self.sort_data(ds)\n",
    "        #After this step, we will use only sorted data\n",
    "        self.missing_values(ds)\n",
    "    \n",
    "    \n",
    "    #Min max scaling \n",
    "    def min_max_scaling(self, ds):\n",
    "        scaler = MinMaxScaler()\n",
    "        #print(ds.org_data.head())\n",
    "        # Apply scaling only to numeric columns\n",
    "        numeric_cols = ds.org_data.select_dtypes(include=['float64', 'int64']).columns\n",
    "        #print(numeric_cols)\n",
    "        ds.org_data[numeric_cols] = scaler.fit_transform(ds.org_data[numeric_cols]) \n",
    "        #print('After applying Min Max Scaling:' ,ds.org_data.head())\n",
    "    \n",
    "    \n",
    "    #This function sets the no of features and class names\n",
    "    #to the Dataset class object\n",
    "    def set_Dataset_features(self, ds):\n",
    "        #set class names\n",
    "        last_column_unique = ds.org_data.iloc[:, -1].unique()\n",
    "        ds.set_classes(last_column_unique)\n",
    "        #print(ds.get_classes())\n",
    "        \n",
    "        #set no of features, because one column is classes\n",
    "        ds.feature_count = ds.org_data.shape[1] -1\n",
    "        #print(ds.feature_count)\n",
    "    \n",
    "                \n",
    "    #splitting data into 10 parts, Here we will\n",
    "    #implement stratified 10 folds\n",
    "    def split_data_regression(self,ds):\n",
    "        \n",
    "        # first we need to make 10 datasets of consectutive points\n",
    "        #print('\\n Printing the results of 10 folds')\n",
    "        datagroupSize = len(ds.sorted_data) // 10  \n",
    "        print('Fold Size', datagroupSize)\n",
    "        dataGroup = []\n",
    "        \n",
    "        #reusing this part from my first project\n",
    "        for i in range(10):\n",
    "            start = i * datagroupSize\n",
    "            end = start + datagroupSize\n",
    "            if i == 9:  \n",
    "                dataGroup.append(ds.sorted_data.iloc[start:])\n",
    "            else:\n",
    "                dataGroup.append(ds.sorted_data.iloc[start:end])\n",
    "        \n",
    "        \n",
    "        #once we have datasets of sorted data, now we will make folds\n",
    "        #with one point from each fold.\n",
    "        folds = [[] for _ in range(10)]\n",
    "        \n",
    "        max_group_size = max(len(group) for group in dataGroup)\n",
    "        print('Max Fold Size: ',max_group_size)\n",
    "        \n",
    "        # For each index up to the maximum group size\n",
    "        for i in range(max_group_size):\n",
    "            # Loop over the 10 groups\n",
    "            for j in range(10):\n",
    "                #print('fold:', j, 'datagroup :' ,j, 'value',  dataGroup[j].iloc[i, -1])\n",
    "                # Check if the current group has enough data for the current index\n",
    "                if i < len(dataGroup[j]):  \n",
    "                    # Append the i-th example from the j-th group into the j-th fold\n",
    "                    folds[j].append(dataGroup[j].iloc[i])\n",
    "\n",
    "                    \n",
    "        #converting folds into Dataframes\n",
    "        final_folds = folds = [pd.DataFrame(fold) for fold in folds]\n",
    "        ds.ten_folds = final_folds\n",
    "        \n",
    "        #for i in range(10):\n",
    "        #    print(f'Fold {i + 1}:')\n",
    "        #    print(final_folds[i].tail())\n",
    "        \n",
    "        #print(\"Ten Fold Prints\")\n",
    "        #for i, fold in enumerate(final_folds):\n",
    "        #    print(f'Fold {i+1} size: {len(fold)}')\n",
    "        #    print(fold.iloc[:, -1].value_counts())\n",
    "    \n",
    "    \n",
    "    #splitting data into 10 parts, Here we will\n",
    "    #implement stratified 10 folds for classification\n",
    "    def split_data_classification(self,ds):\n",
    "        \n",
    "        #In this we first need to split data accordign to classes\n",
    "        #Since we want the ratio of classes to be same in each fold\n",
    "        classwise_data = {}\n",
    "        \n",
    "        for cls in ds.classes:\n",
    "            classwise_data[cls] = ds.sorted_data[ds.sorted_data.iloc[:, -1] == cls]\n",
    "                    \n",
    "        # Now we will be creating the folds containing data from \n",
    "        # each fold of class, making the ratio same with original data\n",
    "        folds = [[] for _ in range(10)]\n",
    "        \n",
    "        for cls, cls_data in classwise_data.items():\n",
    "            \n",
    "            #first divide the classwise data into 10 parts.\n",
    "            clsPartsforFolds = len(cls_data) //10\n",
    "            clsPartsRemain = len(cls_data) % 10\n",
    "            \n",
    "            #print('Number of class data per fold',clsPartsforFolds)\n",
    "            #print('Remaining data left in class',clsPartsRemain)\n",
    "            #put these values in all folds\n",
    "            start = 0\n",
    "            for i in range(10):\n",
    "                if clsPartsRemain != 0:\n",
    "                    # Add 1 to handle remaining data\n",
    "                    end = start + clsPartsforFolds + 1  \n",
    "                    clsPartsRemain -= 1\n",
    "                else:\n",
    "                    end = start + clsPartsforFolds\n",
    "            \n",
    "                # Append class data slice to the respective fold\n",
    "                folds[i].extend(cls_data.iloc[start:end].values.tolist())\n",
    "                start = end\n",
    "            \n",
    "        \n",
    "        #converting folds into Dataframes\n",
    "        final_folds = [pd.DataFrame(fold, columns=ds.sorted_data.columns) for fold in folds]    \n",
    "        ds.ten_folds = final_folds\n",
    "        \n",
    "        #This section is just to check if the fold created are in the same ratio\n",
    "        #for i in range(10):\n",
    "        #    print(f'Fold {i + 1}:')\n",
    "        #    print(final_folds[i].tail())\n",
    "        \n",
    "        for i, fold in enumerate(final_folds):\n",
    "            print(f'Fold {i+1} size: {len(fold)}')\n",
    "            print(fold.iloc[:, -1].value_counts())\n",
    "        \n",
    "    \n",
    "    #added the function to see, if we will need it for any dataset\n",
    "    def one_hot_encoding(self, ds):\n",
    "        #this has first coloumn of categorical data, F, M and I\n",
    "        if(ds.name == 'abalone'):\n",
    "            #save class column and drop it\n",
    "            class_column = ds.org_data.iloc[:, -1]  \n",
    "            ds.org_data = ds.org_data.drop(ds.org_data.columns[-1], axis=1)\n",
    "            \n",
    "            #perform one hot encoding    \n",
    "            first_column_name = ds.org_data.columns[0]\n",
    "            one_hot_encoded = pd.get_dummies(ds.org_data[first_column_name], prefix=first_column_name)\n",
    "            ds.org_data = pd.concat([ds.org_data.drop(columns=[first_column_name]), one_hot_encoded], axis=1)\n",
    "            #append the saved class column to the end again\n",
    "            ds.org_data = pd.concat([ds.org_data, class_column], axis=1)\n",
    "        \n",
    "        #this dataset has first column of manufacturer, and second column of \n",
    "        #model name, which has unique values, so dropping it.\n",
    "        if(ds.name == 'machine'):\n",
    "            \n",
    "            #here i am dropping the last colum, because class / result is \n",
    "            ds.org_data = ds.org_data.drop(ds.org_data.columns[-1], axis=1)\n",
    "            #save the class column and then drop it   \n",
    "            class_column = ds.org_data.iloc[:, -1]\n",
    "            ds.org_data = ds.org_data.drop(ds.org_data.columns[-1], axis=1)\n",
    "            \n",
    "            #drop the second column, which is model number\n",
    "            ds.org_data = ds.org_data.drop(ds.org_data.columns[1], axis=1)\n",
    "            first_column_name = ds.org_data.columns[0]\n",
    "            one_hot_encoded = pd.get_dummies(ds.org_data[first_column_name], prefix=first_column_name)\n",
    "            ds.org_data = pd.concat([ds.org_data.drop(columns=[first_column_name]), one_hot_encoded], axis=1)\n",
    "            \n",
    "            #append the saved class column to the end again\n",
    "            ds.org_data = pd.concat([ds.org_data, class_column], axis=1)\n",
    "        \n",
    "        #first column is months, second column is days\n",
    "        if ds.name == 'forestfires':\n",
    "            \n",
    "            #tranform the last column to log \n",
    "            #*****This is still not helping, we might need normalization*****\n",
    "            ds.org_data['area'] = np.log(ds.org_data['area'] +  1)\n",
    "            \n",
    "            \n",
    "            #save the class column and then drop it, \n",
    "            class_column = ds.org_data.iloc[:, -1]\n",
    "            ds.org_data = ds.org_data.drop(ds.org_data.columns[-1], axis=1)\n",
    "            \n",
    "            first_column_name = ds.org_data.columns[2]\n",
    "            second_column_name = ds.org_data.columns[3]\n",
    "            one_hot_encoded = pd.get_dummies(ds.org_data[[first_column_name, second_column_name]], \n",
    "                                             prefix=[first_column_name, second_column_name])\n",
    "                                    \n",
    "            ds.org_data = pd.concat([ds.org_data.drop(columns=[first_column_name, second_column_name]), \n",
    "                                     one_hot_encoded], axis=1)\n",
    "            \n",
    "            #append the saved class column to the end again\n",
    "            ds.org_data = pd.concat([ds.org_data, class_column], axis=1)\n",
    "        \n",
    "        #print(\"One hot Encoding Step, Resulted data shown below\")\n",
    "        #print(ds.org_data.head())\n",
    "        \n",
    "    #First we need to sort the data, if not sorted according to the \n",
    "    #classes, if data is already sorted, then original values will remain\n",
    "    #same without change\n",
    "    def sort_data(self, ds):\n",
    "        ds.sorted_data = ds.org_data.sort_values(by=ds.org_data.columns[-1])\n",
    "        #and then reset the index\n",
    "        ds.sorted_data = ds.sorted_data.reset_index(drop=True)\n",
    "        \n",
    "        \n",
    "    # this function handles missing values, for this project\n",
    "    # forest fires has no missing values\n",
    "    # Abalone has no missing values\n",
    "    # machine has no missing values\n",
    "    # glass has no missing values\n",
    "    # Soybean has no missing values\n",
    "    # breastCancer has missing values\n",
    "    def missing_values(self, ds):      \n",
    "        # Check if any missing values exist, using '?' and null values\n",
    "        ds.sorted_data.replace('?', np.nan, inplace=True)\n",
    "        has_missing_values = ds.sorted_data.isnull().values.any()\n",
    "        missing_values_count = ds.sorted_data.isnull().sum().sum()\n",
    "        \n",
    "        if (has_missing_values):\n",
    "            print(\"This dataset has missing values\")\n",
    "            if missing_values_count > 1:\n",
    "                # It has missing values  \n",
    "                #print(\"This dataset has more missing values\")\n",
    "                rows_with_missing = dds.sorted_data[ds.sorted_data.isnull().any(axis=1)].index\n",
    "                #print(\"Rows with missing values before filling:\")\n",
    "                #print(ds.ds.sorted_data.loc[rows_with_missing])\n",
    "                \n",
    "                if(ds.name == 'breast-cancer-wisconsin'):\n",
    "                    #print(\"Inside the median if\")\n",
    "                    for column in ds.sorted_data.columns[:-1]:\n",
    "                        median_value = ds.sorted_data[column].median()\n",
    "                        # Filling the missing values\n",
    "                        ds.sorted_data[column].fillna(median_value, inplace=True)\n",
    "    \n",
    "                #print(\"Rows with missing values after filling:\")\n",
    "                #print(ds.sorted_data.loc[rows_with_missing])\n",
    "            else:\n",
    "                ds.sorted_data.dropna(inplace=True)\n",
    "                print(\"Dropped the row\")\n",
    "\n",
    "    #This one function will work for classification    \n",
    "    def split_tuning_data_classification(self, ds):\n",
    "        print('Spliting the data into tunning and remaining')\n",
    "        # Backup sorted data\n",
    "        ds.sorted_data_bk = ds.sorted_data.copy()\n",
    "    \n",
    "        # Initialize empty lists for tuning and remaining data\n",
    "        tuning_data = []\n",
    "        remaining_data = []\n",
    "\n",
    "        # Separate the data into classes\n",
    "        for cls in ds.classes:\n",
    "            #print(cls)\n",
    "            classwise_data = ds.sorted_data[ds.sorted_data.iloc[:, -1] == cls]\n",
    "\n",
    "            # Shuffle the data\n",
    "            classwise_data = classwise_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "        \n",
    "            # Extract 10% tuning set\n",
    "            num_tuning_samples = max(1, int(len(classwise_data) * 0.1))  \n",
    "            #print('number of samples from this class',num_tuning_samples )\n",
    "        \n",
    "            # Append tuning set and remaining data\n",
    "            tuning_data.append(classwise_data.iloc[:num_tuning_samples])\n",
    "            remaining_data.append(classwise_data.iloc[num_tuning_samples:])\n",
    "    \n",
    "        # Concatenate tuning and remaining data\n",
    "        ds.tuning_data = pd.concat(tuning_data).reset_index(drop=True)\n",
    "        ds.sorted_data = pd.concat(remaining_data).reset_index(drop=True)\n",
    "        \n",
    "        print('Length of tuning data:',len(ds.tuning_data) )\n",
    "        print('Length of remaining data:',len(ds.sorted_data) )\n",
    "        print('Length of original data:',len(ds.sorted_data_bk) )\n",
    "\n",
    "        \n",
    "    def split_tuning_data_regression(self, ds):\n",
    "        # Backup sorted data\n",
    "        \n",
    "        print('\\n Spliting the data into tunning and remaining')\n",
    "        tuning_fraction = 0.1\n",
    "        n_bins = 10\n",
    "        ds.sorted_data_bk = ds.sorted_data.copy()\n",
    "\n",
    "        # Bin the target variable into quantiles (or any number of bins)\n",
    "        ds.sorted_data['bins'] = pd.qcut(ds.sorted_data.iloc[:, -1], q=n_bins, labels=False, duplicates='drop')\n",
    "\n",
    "        # Initialize empty lists for tuning and remaining data\n",
    "        tuning_data = []\n",
    "        remaining_data = []\n",
    "\n",
    "        # Separate the data into bins (which act as pseudo-classes for stratification)\n",
    "        for bin_value in ds.sorted_data['bins'].unique():\n",
    "            bin_data = ds.sorted_data[ds.sorted_data['bins'] == bin_value]\n",
    "\n",
    "            # Shuffle the data\n",
    "            bin_data = bin_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "            # Extract the specified fraction (10% by default) for the tuning set\n",
    "            num_tuning_samples = int(len(bin_data) * tuning_fraction)\n",
    "\n",
    "            # Append tuning set and remaining data\n",
    "            tuning_data.append(bin_data.iloc[:num_tuning_samples])\n",
    "            remaining_data.append(bin_data.iloc[num_tuning_samples:])\n",
    "\n",
    "        # Concatenate tuning and remaining data\n",
    "        ds.tuning_data = pd.concat(tuning_data).reset_index(drop=True)\n",
    "        ds.sorted_data = pd.concat(remaining_data).reset_index(drop=True)\n",
    "\n",
    "        # Drop the 'bins' column, as it's not needed anymore\n",
    "        ds.sorted_data.drop(columns=['bins'], inplace=True)\n",
    "        ds.tuning_data.drop(columns=['bins'], inplace=True)\n",
    "\n",
    "        # Check lengths\n",
    "        print('Length of tuning data:', len(ds.tuning_data))\n",
    "        print('Length of remaining data:', len(ds.sorted_data))\n",
    "        print('Length of original data:', len(ds.sorted_data_bk))    \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c67288d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict,Counter\n",
    "import numpy as np\n",
    "\n",
    "class KNN_Classification:\n",
    "    def __init__(self, k=5):\n",
    "        self.k = k\n",
    "\n",
    "    def train(self, ds):\n",
    "        #all columns except for the last, of all rows\n",
    "        self.train_features = ds.train_data.iloc[:, :-1]\n",
    "        #only last column of all rows\n",
    "        self.train_cls = ds.train_data.iloc[:, -1]\n",
    "        \n",
    "            \n",
    "    #testign the model\n",
    "    def test(self,ds):\n",
    "        #all columns except for the last, of all rows\n",
    "        self.test_features = ds.test_data.iloc[:, :-1]\n",
    "        #only last column of all rows\n",
    "        self.test_cls = ds.test_data.iloc[:, -1]\n",
    "        test_results = []\n",
    "        \n",
    "        for test in self.test_features.values:\n",
    "            #store the distance for this test set\n",
    "            distances =[]\n",
    "            \n",
    "            #for this test row, calculate the distance with each train row\n",
    "            for train in self.train_features.values:\n",
    "                #euclidean distance \n",
    "                distance = np.linalg.norm(train - test)\n",
    "                distances.append(distance)\n",
    "                \n",
    "            #print(\"Distances: \" , distances)    \n",
    "            # Find indices of the k nearest neighbors\n",
    "            k_indices = np.argsort(distances)[:self.k]\n",
    "            \n",
    "            \n",
    "            #print(\"Indexes of these distnces: \" , k_indices)\n",
    "            # According to indices get the class from train set \n",
    "            k_nearest_labels = [self.train_cls.iloc[i] for i in k_indices]\n",
    "            \n",
    "            \n",
    "            #print(\"Labels : \" , k_nearest_labels)\n",
    "            # find the most common labels among the k neighbors\n",
    "            # as asked for classification\n",
    "            most_common = Counter(k_nearest_labels).most_common(1)\n",
    "            \n",
    "            #print(\"Most common label : \" , most_common)\n",
    "            # predicted label for this test row\n",
    "            predicted_label = most_common[0][0]\n",
    "            test_results.append(predicted_label)\n",
    "        \n",
    "        results_df = pd.DataFrame({\n",
    "        'Actual Class': self.test_cls.tolist(),   \n",
    "        'Predicted Class': test_results           \n",
    "        })\n",
    "\n",
    "\n",
    "        return results_df\n",
    "    \n",
    "    \n",
    "    #first prepare the test and train data from folds\n",
    "    #then call train and test 10 times, and \n",
    "    #take the average to prepare the results\n",
    "    def mainFunction(self, ds):   \n",
    "        #adding combined results for each fold\n",
    "        combined_results = pd.DataFrame()\n",
    "        #combined_results = pd.DataFrame()\n",
    "        #rs = Results()\n",
    "        eval_result_dic = {}\n",
    "        no_folds = 10\n",
    "        \n",
    "        for i in range(10):\n",
    "            # one fold for testing\n",
    "            print(f'Fold: {i}')\n",
    "            ds.test_data = ds.ten_folds[i]            \n",
    "            # remaining 9 folds for training\n",
    "            ds.train_data = pd.concat([ds.ten_folds[j] for j in range(10) if j != i])\n",
    "            \n",
    "            self.train(ds)\n",
    "            results = self.test(ds)\n",
    "            print(results)\n",
    "\n",
    "            \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60be912",
   "metadata": {},
   "source": [
    "# KNN Regression Class: Demonstrate one example and Average over 10 Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ebdc8a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict,Counter\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class KNN_Regression:\n",
    "    def __init__(self, k=5):\n",
    "        self.k = k\n",
    "\n",
    "    def train(self, ds):\n",
    "        # All columns except for the last, of all rows\n",
    "        self.train_features = ds.train_data.iloc[:, :-1]\n",
    "        # Only last column of all rows\n",
    "        self.train_cls = ds.train_data.iloc[:, -1]\n",
    "\n",
    "    def test(self, ds):\n",
    "        # All columns except for the last, of all rows\n",
    "        self.test_features = ds.test_data.iloc[:, :-1]\n",
    "        # Only last column of all rows\n",
    "        self.test_cls = ds.test_data.iloc[:, -1]\n",
    "        test_results = []\n",
    "\n",
    "        for test in self.test_features.values:\n",
    "            # Calculate the distance from the test point to all training points\n",
    "            distances = [np.linalg.norm(train - test) for train in self.train_features.values]\n",
    "\n",
    "            # Find indices of the k nearest neighbors\n",
    "            k_indices = np.argsort(distances)[:self.k]\n",
    "            #print(\"Indexes of distnces: \" , k_indices)\n",
    "            \n",
    "            # Get the k nearest neighbor values from the training labels\n",
    "            k_nearest_values = np.array([self.train_cls.iloc[i] for i in k_indices])\n",
    "            #print(\"Values of nearest neighbours: \" , k_nearest_values)\n",
    "            \n",
    "            # Get the k nearest neighbor distances\n",
    "            k_nearest_distances = np.array([distances[i] for i in k_indices])\n",
    "\n",
    "            # Compute inverse distance weights for the k nearest neighbors\n",
    "            # I started with basic mean, but then added this (weighted average)\n",
    "            # afetr checking online this will give better results \n",
    "            weights = []\n",
    "            for distance in k_nearest_distances:\n",
    "                if distance == 0:\n",
    "                    weights.append(1e10)  \n",
    "                else:\n",
    "                    weights.append(1 / distance)  \n",
    "\n",
    "            weights = np.array(weights)\n",
    "\n",
    "            # Calculate the weighted sum for the predicted value\n",
    "            weighted_sum = np.dot(k_nearest_values, weights) / np.sum(weights)\n",
    "            #print(\"Weighted average of these values : \" , weighted_sum)\n",
    "            \n",
    "            # Store the predicted value\n",
    "            test_results.append(weighted_sum)\n",
    "\n",
    "        # Create a results DataFrame\n",
    "        results_df = pd.DataFrame({\n",
    "            'Actual Class': self.test_cls.tolist(),\n",
    "            'Predicted Class': test_results\n",
    "        })\n",
    "        \n",
    "        return results_df\n",
    "    \n",
    "    #This is just for the video\n",
    "    def print_oneResult(self, ds):\n",
    "        \n",
    "        print('\\n Printing the results of one data point')\n",
    "        # All columns except for the last, of all rows\n",
    "        self.test_features = ds.test_data.iloc[:, :-1]\n",
    "        # Only last column of all rows\n",
    "        self.test_cls = ds.test_data.iloc[:, -1]\n",
    "        test_results = []\n",
    "\n",
    "        for test in self.test_features.values:\n",
    "            # Calculate the distance from the test point to all training points\n",
    "            distances = [np.linalg.norm(train - test) for train in self.train_features.values]\n",
    "\n",
    "            # Find indices of the k nearest neighbors\n",
    "            k_indices = np.argsort(distances)[:self.k]\n",
    "            print(\"Indexes of distnces: \" , k_indices)\n",
    "            \n",
    "            # Get the k nearest neighbor values from the training labels\n",
    "            k_nearest_values = np.array([self.train_cls.iloc[i] for i in k_indices])\n",
    "            print(\"Values of nearest neighbours: \" , k_nearest_values)\n",
    "            \n",
    "            # Get the k nearest neighbor distances\n",
    "            k_nearest_distances = np.array([distances[i] for i in k_indices])\n",
    "\n",
    "            # Compute inverse distance weights for the k nearest neighbors\n",
    "            # I started with basic mean, but then added this (weighted average)\n",
    "            # afetr checking online this will give better results \n",
    "            weights = []\n",
    "            for distance in k_nearest_distances:\n",
    "                if distance == 0:\n",
    "                    weights.append(1e10)  \n",
    "                else:\n",
    "                    weights.append(1 / distance)  \n",
    "\n",
    "            weights = np.array(weights)\n",
    "\n",
    "            # Calculate the weighted sum for the predicted value\n",
    "            weighted_sum = np.dot(k_nearest_values, weights) / np.sum(weights)\n",
    "            print(\"Weighted average of these values, which is the Prediction : \" , weighted_sum)\n",
    "            break\n",
    "        print('\\n')\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Visualize modelâ€™s predictions in comparison to the true values\n",
    "    def graph_results(self, result_df):\n",
    "        \n",
    "        plt.figure(figsize=(5, 5))\n",
    "    \n",
    "        plt.scatter(result_df['Actual Class'], result_df['Predicted Class'])\n",
    "    \n",
    "        # Adjust the scale based on the actual data range\n",
    "        min_val = min(result_df['Actual Class'].min(), result_df['Predicted Class'].min())\n",
    "        max_val = max(result_df['Actual Class'].max(), result_df['Predicted Class'].max())\n",
    "    \n",
    "        # Plot the perfect prediction line with a dynamic range\n",
    "        plt.plot([min_val, max_val], [min_val, max_val], '--k', label=\"Correct prediction\")\n",
    "\n",
    "        plt.xlabel('Actual Values')\n",
    "        plt.ylabel('Predicted Values')\n",
    "        plt.title(\"Acrual vs Predicted \")\n",
    "    \n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    #first prepare the test and train data from folds\n",
    "    #then call train and test 10 times, and \n",
    "    #take the average to prepare the results\n",
    "    def mainFunction(self, ds):\n",
    "        print('\\n')                  \n",
    "        #adding combined results for each fold\n",
    "        combined_results = pd.DataFrame()\n",
    "        #rs = Results()\n",
    "        no_folds = 10\n",
    "        print('K :', self.k)\n",
    "        fold_errors = []\n",
    "        \n",
    "        \n",
    "        #just printing one point example\n",
    "        ds.test_data = ds.ten_folds[1]\n",
    "        ds.train_data = pd.concat(ds.ten_folds[:9])\n",
    "        self.print_oneResult(ds)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range(no_folds):\n",
    "            # one fold for testing\n",
    "            \n",
    "            print(f'Fold: {i}')\n",
    "            \n",
    "            ds.test_data = ds.ten_folds[i]            \n",
    "            # remaining 9 folds for training\n",
    "            ds.train_data = pd.concat([ds.ten_folds[j] for j in range(10) if j != i])\n",
    "            \n",
    "            self.train(ds)\n",
    "            results = self.test(ds)\n",
    "            #print(results)\n",
    "            \n",
    "            # Calculate MAE for the current fold\n",
    "            mae = self.mean_absolute_error(results['Actual Class'], results['Predicted Class'])\n",
    "            print(f'Fold: {i}, Mean Absolute Error: {mae}')\n",
    "            fold_errors.append(mae)\n",
    "            #this is just for printing the graph for report\n",
    "            combined_results = pd.concat([combined_results, results], ignore_index=True)\n",
    "\n",
    "        # Average MAE over the folds\n",
    "        average_mae = sum(fold_errors) / len(fold_errors)\n",
    "        print(f'KNN Standard: Average Mean Absolute Error over all Folds: {average_mae}')\n",
    "        self.graph_results(combined_results)\n",
    "        \n",
    "            \n",
    "    #we have 10% tuning data in ds.tuning_data\n",
    "    #and remaining 90% is split into 10 folds, ds.ten_folds            \n",
    "    def tuning_k(self, ds):\n",
    "        \n",
    "       \n",
    "        eval_result_dic = {}\n",
    "    \n",
    "        k_values =[1,2,3,4,5,6,7,8,9, 10, 11, 12, 13, 14, 15]\n",
    "        best_k = 1\n",
    "        best_score = float('inf')          \n",
    "        no_folds = 10\n",
    "        K_results = {}\n",
    "        \n",
    "        for k in k_values:\n",
    "            #print(f'K: {k}')\n",
    "            self.k = k\n",
    "            fold_errors = []\n",
    "            \n",
    "            for i in range(10):\n",
    "                # one fold for testing\n",
    "                #print(f'Fold: {i}')\n",
    "                ds.test_data = ds.tuning_data            \n",
    "                # remaining 9 folds for training\n",
    "                ds.train_data = pd.concat([ds.ten_folds[j] for j in range(10) if j != i])\n",
    "            \n",
    "                self.train(ds)\n",
    "                results = self.test(ds)\n",
    "                \n",
    "                # Calculate MAE for the current fold\n",
    "                mae = self.mean_absolute_error(results['Actual Class'], results['Predicted Class'])\n",
    "                #print(f'Fold: {i}, Mean Absolute Error: {mae}')\n",
    "                fold_errors.append(mae)\n",
    "\n",
    "            # Average MAE over the folds\n",
    "            average_mae = sum(fold_errors) / len(fold_errors)\n",
    "            #print(f'K: {k}, Average Mean Absolute Error over all Folds: {average_mae}')\n",
    "            K_results[k] = average_mae\n",
    "\n",
    "            # Check for the best k\n",
    "            if average_mae < best_score:\n",
    "                best_score = average_mae\n",
    "                best_k = k\n",
    "                \n",
    "        print('\\n Tunning Process Result')    \n",
    "        print(f'Best k: {best_k} with MAE: {best_score}')\n",
    "        self.k = best_k\n",
    "    \n",
    "\n",
    "    def mean_absolute_error(self, actual, predicted):\n",
    "        return np.mean(np.abs(np.array(actual) - np.array(predicted)))          \n",
    "                \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d624620",
   "metadata": {},
   "source": [
    "# Demonstration of kernel Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3b027f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#once we have gaussian regression, then we will call this class\n",
    "class KNN_Gaussian_Regression:\n",
    "    def __init__(self, centroids, centroid_values, k=5, sigma=1.0):\n",
    "        self.k = k\n",
    "        #parameter for the Gaussian kernel\n",
    "        self.bandwidth = sigma\n",
    "        self.centroids = centroids\n",
    "        self.centroid_values = centroid_values\n",
    "\n",
    "    # Test function using centroids\n",
    "    def test(self, ds):\n",
    "        # Test data (all columns except the last one)\n",
    "        self.test_features = ds.test_data.iloc[:, :-1]\n",
    "        # Actual target values (last column)\n",
    "        self.test_cls = ds.test_data.iloc[:, -1]\n",
    "\n",
    "        test_results = []\n",
    "\n",
    "        for test in self.test_features.values:\n",
    "            # Calculate the distance from the test point to all centroids\n",
    "            distances = [np.linalg.norm(centroid - test) for centroid in self.centroids]\n",
    "\n",
    "            # Find indices of the k nearest centroids\n",
    "            k_indices = np.argsort(distances)[:self.k]\n",
    "\n",
    "            # Get the k nearest neighbor values from the centroid_values\n",
    "            k_nearest_values = np.array([self.centroid_values[i] for i in k_indices])\n",
    "\n",
    "            # Get the distances of the k-nearest centroids\n",
    "            k_nearest_distances = np.array([distances[i] for i in k_indices])\n",
    "\n",
    "            # Compute Gaussian kernel weights based on the distances\n",
    "            # this is the part which is different from the standard KNN\n",
    "            \n",
    "            weights = np.exp(- (k_nearest_distances ** 2) / (2 * self.bandwidth ** 2))\n",
    "\n",
    "            # Avoid division by zero by checking if the sum of weights is zero\n",
    "            weight_sum = np.sum(weights)\n",
    "            if weight_sum == 0:\n",
    "                # If weights are all zero, use the mean of k nearest values\n",
    "                weighted_sum = np.mean(k_nearest_values)\n",
    "            else:\n",
    "                # Calculate the weighted sum for the predicted value\n",
    "                weighted_sum = np.dot(k_nearest_values, weights) / weight_sum\n",
    "\n",
    "            # Store the predicted value\n",
    "            test_results.append(weighted_sum)\n",
    "\n",
    "        # Create a results DataFrame\n",
    "        results_df = pd.DataFrame({\n",
    "            'Actual Class': self.test_cls.tolist(),\n",
    "            'Predicted Class': test_results\n",
    "        })\n",
    "\n",
    "        return results_df\n",
    "        \n",
    "    # first prepare the test and train data from folds\n",
    "    # then call train and test 10 times, and\n",
    "    # take the average to prepare the results\n",
    "    def mainFunction(self, ds):\n",
    "        # Adding combined results for each fold\n",
    "        combined_results = pd.DataFrame()\n",
    "        no_folds = 10\n",
    "        print('K:', self.k)\n",
    "        fold_errors = []\n",
    "\n",
    "        for i in range(no_folds):\n",
    "            # one fold for testing\n",
    "            print(f'Fold: {i}')\n",
    "            ds.test_data = ds.ten_folds[i]\n",
    "            results = self.test(ds)\n",
    "\n",
    "            # Calculate MAE for the current fold\n",
    "            mae = self.mean_absolute_error(results['Actual Class'], results['Predicted Class'])\n",
    "            print(f'Fold: {i}, Mean Absolute Error: {mae}')\n",
    "            fold_errors.append(mae)\n",
    "\n",
    "        # Average MAE over the folds\n",
    "        average_mae = sum(fold_errors) / len(fold_errors)\n",
    "        print(f' KNN Clustering: Average Mean Absolute Error over all Folds: {average_mae}')\n",
    "        return average_mae\n",
    "\n",
    "    def tuning_bandwidth(self, ds):\n",
    "        # Bandwidth values to tune\n",
    "        bandwidth_values = [0.1, 0.5, 1, 2, 5, 10]\n",
    "        best_bandwidth = 0.1\n",
    "        best_score = float('inf')\n",
    "\n",
    "        for bandwidth in bandwidth_values:\n",
    "            print(f'Tuning with bandwidth: {bandwidth}')\n",
    "            self.bandwidth = bandwidth\n",
    "            fold_errors = []\n",
    "\n",
    "            for i in range(10):\n",
    "                ds.test_data = ds.tuning_data\n",
    "                ds.train_data = pd.concat([ds.ten_folds[j] for j in range(10) if j != i])\n",
    "\n",
    "                results = self.test(ds)\n",
    "\n",
    "                # Calculate MAE for the current fold\n",
    "                mae = self.mean_absolute_error(results['Actual Class'], results['Predicted Class'])\n",
    "                fold_errors.append(mae)\n",
    "\n",
    "            # Average MAE over the folds\n",
    "            average_mae = sum(fold_errors) / len(fold_errors)\n",
    "            print(f'Bandwidth: {bandwidth}, Average MAE: {average_mae}')\n",
    "\n",
    "            # Update the best bandwidth based on performance\n",
    "            if average_mae < best_score:\n",
    "                best_score = average_mae\n",
    "                best_bandwidth = bandwidth\n",
    "\n",
    "        print(f'Best bandwidth: {best_bandwidth} with MAE: {best_score}')\n",
    "        self.bandwidth = best_bandwidth\n",
    "\n",
    "    def mean_absolute_error(self, actual, predicted):\n",
    "        return np.mean(np.abs(np.array(actual) - np.array(predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63e80d0",
   "metadata": {},
   "source": [
    "# Eddited KNN Class: Demonstation of Error Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7254c3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EditedKNNRegression:\n",
    "    \n",
    "    def __init__(self, k=5, error_threshold=1.0):\n",
    "        self.k = k\n",
    "        self.error_threshold = error_threshold\n",
    "        \n",
    "    def knn_predict(self, test_point):\n",
    "        # Ensure train_features is not empty\n",
    "        if self.train_features.empty:\n",
    "            print(\"Error: No training data available for prediction.\")\n",
    "            return np.nan\n",
    "\n",
    "        # Calculate distances from the test point to all training points\n",
    "        distances = np.linalg.norm(self.train_features.values - test_point, axis=1)\n",
    "        # Get the indices of the k nearest neighbors\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        \n",
    "        # Ensure that we're not working with an empty slice\n",
    "        if len(k_indices) == 0:\n",
    "            print(\"Error: No neighbors found for the test point.\")\n",
    "            return np.nan\n",
    "\n",
    "        # Predict value based on k nearest neighbors\n",
    "        return np.mean(self.train_cls.values[k_indices])\n",
    "    \n",
    "    \n",
    "    def eddited_knn_one_sample(self, ds):\n",
    "\n",
    "        print('\\n One Example for edditing error removal')\n",
    "        previous_performance = float('inf')\n",
    "        \n",
    "        self.train_features = ds.train_data.iloc[:, :-1]\n",
    "        self.train_cls = ds.train_data.iloc[:, -1]\n",
    "        \n",
    "        while True:\n",
    "            errors = []\n",
    "            global_flag = False\n",
    "            \n",
    "            for i in range(len(self.train_features)):\n",
    "                test_point = self.train_features.iloc[i].values\n",
    "                predicted_value = self.knn_predict(test_point)\n",
    "                \n",
    "                \n",
    "                # Skip if prediction failed (NaN)\n",
    "                if np.isnan(predicted_value):\n",
    "                    continue\n",
    "                \n",
    "                actual_value = self.train_cls.iloc[i]\n",
    "    \n",
    "                error = abs(predicted_value - actual_value)\n",
    "                \n",
    "                flag = False\n",
    "                if error > 0.01:\n",
    "                    print('Actual value: ', actual_value)\n",
    "                    print('Predicted value: ', predicted_value)\n",
    "                    print('Error:', error)\n",
    "                    print('Error is greater than threshold, so removing this value from dataset')\n",
    "                    errors.append(i)\n",
    "                    flag = True\n",
    "                    global_flag = True\n",
    "                    \n",
    "                if(flag):\n",
    "                    break\n",
    "            if(global_flag):\n",
    "                break\n",
    "        print('\\n')\n",
    "\n",
    "    \n",
    "    def edited_knn(self, ds):\n",
    "        previous_performance = float('inf')\n",
    "        \n",
    "        self.train_features = ds.train_data.iloc[:, :-1]\n",
    "        self.train_cls = ds.train_data.iloc[:, -1]\n",
    "        \n",
    "        while True:\n",
    "            errors = []\n",
    "            \n",
    "            for i in range(len(self.train_features)):\n",
    "                test_point = self.train_features.iloc[i].values\n",
    "                predicted_value = self.knn_predict(test_point)\n",
    "                \n",
    "                # Skip if prediction failed (NaN)\n",
    "                if np.isnan(predicted_value):\n",
    "                    continue\n",
    "                \n",
    "                actual_value = self.train_cls.iloc[i]\n",
    "                error = abs(predicted_value - actual_value)\n",
    "                \n",
    "                if error > self.error_threshold:\n",
    "                    errors.append(i)\n",
    "            \n",
    "            if not errors:\n",
    "                break\n",
    "            \n",
    "            self.train_features = self.train_features.drop(index=self.train_features.index[errors])\n",
    "            self.train_cls = self.train_cls.drop(index=self.train_cls.index[errors])\n",
    "            \n",
    "            if self.train_features.empty or self.train_cls.empty:\n",
    "                print(\"Warning: Training set is empty after editing. Stopping.\")\n",
    "                break\n",
    "            \n",
    "            current_performance = np.mean(np.abs(self.train_cls - self.knn_predict(self.train_features.values)))\\\n",
    "\n",
    "            # Performance degradation check with a threshold (e.g., 0.01)\n",
    "            # this is the additional step, i had to add for the regression\n",
    "            # for classification, this will be differnt\n",
    "            if (previous_performance - current_performance) < 0.01:\n",
    "                break\n",
    "            \n",
    "            previous_performance = current_performance\n",
    "            \n",
    "        #print(f'Number of examples remaining after editing: {len(self.train_features)}')\n",
    "        self.no_examples = len(self.train_features)\n",
    "        \n",
    "\n",
    "    def mean_absolute_error(self, actual, predicted):\n",
    "        #had to add checks else after edditing, I was runnign into \n",
    "        # empty datasets and thus predictions were nan\n",
    "        if len(actual) == 0 or len(predicted) == 0:\n",
    "            print(\"Error: Actual or predicted values are empty.\")\n",
    "            return np.nan\n",
    "\n",
    "        if np.isnan(predicted).any() or np.isnan(actual).any():\n",
    "            print(\"Warning: NaN values found in predictions or actual values.\")\n",
    "            return np.nan\n",
    "        \n",
    "        return np.mean(np.abs(np.array(actual) - np.array(predicted)))\n",
    "    \n",
    "    \n",
    "    def mainFunction(self, ds):\n",
    "        print('\\n Eddited KNN Main Fuction')\n",
    "        no_folds = 10\n",
    "        fold_errors = []\n",
    "        combined_results = pd.DataFrame()\n",
    "        \n",
    "        self.eddited_knn_one_sample(ds)\n",
    "        \n",
    "\n",
    "        for i in range(no_folds):\n",
    "            \n",
    "            print(f'Fold: {i}')\n",
    "            ds.test_data = ds.ten_folds[i]\n",
    "            ds.train_data = pd.concat([ds.ten_folds[j] for j in range(10) if j != i])\n",
    "            \n",
    "            if ds.train_data.empty:\n",
    "                print(\"Error: Training data is empty. Skipping this fold.\")\n",
    "                continue\n",
    "\n",
    "            self.edited_knn(ds)\n",
    "            predictions = [self.knn_predict(test_point) for test_point in ds.test_data.iloc[:, :-1].values]\n",
    "\n",
    "            actual_values = ds.test_data.iloc[:, -1]\n",
    "            mae = self.mean_absolute_error(actual_values, predictions)\n",
    "            fold_errors.append(mae)\n",
    "            print(f'Fold {i}, Mean Absolute Error: {mae}')\n",
    "            \n",
    "\n",
    "        average_mae = sum(fold_errors) / len(fold_errors)\n",
    "        print(f'KNN Eddited: Average MAE across all folds: {average_mae}')\n",
    "        return average_mae\n",
    "    \n",
    "    \n",
    "    def tune_k_and_threshold(self, ds):\n",
    "        print( \" \\nTunning Part for Eddited KNN \")\n",
    "        error_thresholds = np.arange(0.1, 5.0, 0.1)\n",
    "        best_k = self.k\n",
    "        #check for k values 1 to 5\n",
    "        k_values = range(1, 6)\n",
    "        best_error_threshold = self.error_threshold\n",
    "        best_performance = float('inf')\n",
    "\n",
    "        for k in k_values:\n",
    "            for error_threshold in error_thresholds:\n",
    "                self.k = k\n",
    "                self.error_threshold = error_threshold\n",
    "                # Reset fold errors for each new k and threshold combination\n",
    "                fold_errors = []  \n",
    "                \n",
    "                # Track the number of valid folds\n",
    "                valid_folds = 0  \n",
    "\n",
    "                for i in range(10):\n",
    "                    ds.test_data = ds.tuning_data\n",
    "                    ds.train_data = pd.concat([ds.ten_folds[j] for j in range(10) if j != i])\n",
    "\n",
    "                    if ds.train_data.empty or ds.test_data.empty:\n",
    "                        #print(\"Error: One of the folds is empty. Skipping this fold.\")\n",
    "                        continue\n",
    "\n",
    "                    self.edited_knn(ds)\n",
    "\n",
    "                    # Check if training set became empty after editing\n",
    "                    if ds.train_data.empty:\n",
    "                        #print(f\"Warning: Training set is empty after editing. Stopping for k={k}, threshold={error_threshold}.\")\n",
    "                        break\n",
    "\n",
    "                    predictions = [self.knn_predict(point) for point in ds.test_data.iloc[:, :-1].values]\n",
    "\n",
    "                    # Ensure no NaN predictions or actual values\n",
    "                    if np.isnan(predictions).any() or np.isnan(ds.test_data.iloc[:, -1]).any():\n",
    "                        #print(\"Warning: NaN values found in predictions or actual values. Skipping this fold.\")\n",
    "                        continue\n",
    "\n",
    "                    current_performance = self.mean_absolute_error(ds.test_data.iloc[:, -1], predictions)\n",
    "                    fold_errors.append(current_performance)\n",
    "                    valid_folds += 1\n",
    "\n",
    "                # Calculate average performance if valid folds are available\n",
    "                if valid_folds > 0:\n",
    "                    average_performance = np.mean(fold_errors)\n",
    "                    #print(f'K: {k}, Error Threshold: {error_threshold}, Average MAE: {average_performance}')\n",
    "\n",
    "                    # Update the best k and threshold based on performance\n",
    "                    if average_performance < best_performance:\n",
    "                        best_performance = average_performance\n",
    "                        best_k = k\n",
    "                        best_error_threshold = error_threshold\n",
    "                else:\n",
    "                    print(f\"Warning: No valid folds for k={k}, threshold={error_threshold}. Skipping this combination.\")\n",
    "\n",
    "        # Set the best k and error threshold found during tuning\n",
    "        self.k = best_k\n",
    "        self.error_threshold = best_error_threshold\n",
    "        print(f'Best k: {best_k}, Best Error Threshold: {best_error_threshold}, Performance: {best_performance}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb6679e",
   "metadata": {},
   "source": [
    "# Clustering Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d0ce71c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class KMeansClustering:\n",
    "\n",
    "    def __init__(self, kc=10,distance_type='euclidean'):\n",
    "        self.k = kc\n",
    "        self.distance_type = distance_type\n",
    "        # To store the centroids\n",
    "        self.centroids = None\n",
    "        # To store the labels of the clusters\n",
    "        self.labels = None  \n",
    "\n",
    "    def Centroid_initializer(self, X):\n",
    "        # Initialize centroids randomly from the dataset\n",
    "        centroids = X[np.random.choice(X.shape[0], self.k, replace=False)]\n",
    "        return centroids\n",
    "\n",
    "    def calculate_distance(self, vector1, vector2):\n",
    "        # Convert the vectors to numpy arrays\n",
    "        vector1 = np.array(vector1)\n",
    "        vector2 = np.array(vector2)\n",
    "\n",
    "        # Calculate the distance based on the specified type\n",
    "        if self.distance_type.lower() == 'euclidean':\n",
    "            distance = np.linalg.norm(vector1 - vector2)\n",
    "        elif self.distance_type.lower() == 'manhattan':\n",
    "            distance = np.sum(np.abs(vector1 - vector2))\n",
    "        else:\n",
    "            raise ValueError(\"Invalid distance type. Choose 'euclidean' or 'manhattan'.\")\n",
    "\n",
    "        return distance\n",
    "\n",
    "    def calculate_distance_to_centroids(self, vector1, centroids):\n",
    "        distances = []  # Initialize an empty list to store distances\n",
    "        for centroid in centroids:\n",
    "            # Calculate the distance between the point and the current centroid\n",
    "            distance = self.calculate_distance(vector1, centroid)\n",
    "            # Append the calculated distance to the distances list\n",
    "            distances.append(distance)\n",
    "        return distances\n",
    "\n",
    "    def assign_clusters(self, X, centroids):\n",
    "        # Assign each data point to the nearest centroid\n",
    "        labels = []\n",
    "        for x in X:\n",
    "            distances = self.calculate_distance_to_centroids(x, centroids)\n",
    "            # Find the index of the nearest centroid\n",
    "            nearest_centroid = np.argmin(distances)\n",
    "            labels.append(nearest_centroid)\n",
    "        return np.array(labels)\n",
    "\n",
    "    def update_centroids(self, X, labels):\n",
    "        # Update the centroids by computing the mean of all data points assigned to each centroid\n",
    "        new_centroids = []\n",
    "        for i in range(self.k):\n",
    "            # Get all points assigned to the ith centroid\n",
    "            points_in_cluster = X[labels == i]\n",
    "            # Calculate the mean of those points\n",
    "            new_centroid = np.mean(points_in_cluster, axis=0)\n",
    "            new_centroids.append(new_centroid)\n",
    "        return np.array(new_centroids)\n",
    "\n",
    "    def fit(self, ds):\n",
    "        \n",
    "        max_iter = 100\n",
    "        tol = 1e-4\n",
    "\n",
    "        # All columns except for the last, of all rows\n",
    "        self.train_features = ds.train_data.iloc[:, :-1]\n",
    "        # Only last column of all rows\n",
    "        self.train_cls = ds.train_data.iloc[:, -1]\n",
    "\n",
    "        # Convert to NumPy array\n",
    "        X = self.train_features.to_numpy()\n",
    "\n",
    "        # Initialize centroids\n",
    "        self.centroids = self.Centroid_initializer(X)\n",
    "\n",
    "        for i in range(max_iter):\n",
    "            # Assign each point to the nearest centroid\n",
    "            self.labels = self.assign_clusters(X, self.centroids)\n",
    "\n",
    "            # Update centroids based on current cluster assignments\n",
    "            new_centroids = self.update_centroids(X, self.labels)\n",
    "\n",
    "            # Check for convergence (if centroids don't change significantly)\n",
    "            if np.linalg.norm(new_centroids - self.centroids) < tol:\n",
    "                break\n",
    "\n",
    "            # Update centroids\n",
    "            self.centroids = new_centroids\n",
    "            \n",
    "            \n",
    "    #for regession I have to tune kc because it number of examples\n",
    "    # left from eddited KNN are very lareg\n",
    "    def tuning_kc(self, ds):\n",
    "        best_kc = None\n",
    "        #small mae\n",
    "        best_mae = float('inf') \n",
    "        kc_values = [2, 3, 4, 5, 10, 15]  \n",
    "        \n",
    "        for kc in kc_values:\n",
    "            self.k = kc\n",
    "            ds.train_data = ds.tuning_data\n",
    "            self.fit(ds)\n",
    "            #now we have centroids and \n",
    "            kgr = KNN_Gaussian_Regression(centroids=self.centroids, centroid_values=self.labels)\n",
    "            fold_mae = kgr.mainFunction(ds)\n",
    "            \n",
    "            print(f\"kc = {kc}, MAE = {fold_mae}\")\n",
    "    \n",
    "            # Track the best kc based on the lowest MAE\n",
    "            if fold_mae < best_mae:\n",
    "                best_mae = fold_mae\n",
    "                best_kc = kc\n",
    "                \n",
    "        print('\\n')\n",
    "        print(f\"Best kc: {best_kc}, with MAE: {best_mae}\")\n",
    "        self.k = best_kc\n",
    "\n",
    "    # this is the main function to run all tests from here\n",
    "    def mainFunction(self, ds):\n",
    "        \n",
    "        no_folds = 10\n",
    "        print('K :', self.k)\n",
    "        fold_errors = []\n",
    "        \n",
    "        ds.train_data = pd.concat(ds.ten_folds[:9])\n",
    "        self.fit(ds)\n",
    "        \n",
    "        unique, counts = np.unique(self.labels, return_counts=True)\n",
    "        cluster_sizes = dict(zip(unique, counts))\n",
    "        print(\"Cluster Sizes:\", cluster_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a726d9d7",
   "metadata": {},
   "source": [
    "# Main Driver Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "be0898a5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ten Fold Demonstration\n",
      "Spliting the data into tunning and remaining\n",
      "Length of tuning data: 19\n",
      "Length of remaining data: 195\n",
      "Length of original data: 214\n",
      "Fold 1 size: 22\n",
      "1.0    7\n",
      "2.0    7\n",
      "7.0    3\n",
      "3.0    2\n",
      "5.0    2\n",
      "6.0    1\n",
      "Name: 10, dtype: int64\n",
      "Fold 2 size: 22\n",
      "1.0    7\n",
      "2.0    7\n",
      "7.0    3\n",
      "3.0    2\n",
      "5.0    2\n",
      "6.0    1\n",
      "Name: 10, dtype: int64\n",
      "Fold 3 size: 21\n",
      "1.0    7\n",
      "2.0    7\n",
      "7.0    3\n",
      "3.0    2\n",
      "5.0    1\n",
      "6.0    1\n",
      "Name: 10, dtype: int64\n",
      "Fold 4 size: 20\n",
      "2.0    7\n",
      "1.0    6\n",
      "7.0    3\n",
      "3.0    2\n",
      "5.0    1\n",
      "6.0    1\n",
      "Name: 10, dtype: int64\n",
      "Fold 5 size: 20\n",
      "2.0    7\n",
      "1.0    6\n",
      "7.0    3\n",
      "3.0    2\n",
      "5.0    1\n",
      "6.0    1\n",
      "Name: 10, dtype: int64\n",
      "Fold 6 size: 20\n",
      "2.0    7\n",
      "1.0    6\n",
      "7.0    3\n",
      "3.0    2\n",
      "5.0    1\n",
      "6.0    1\n",
      "Name: 10, dtype: int64\n",
      "Fold 7 size: 19\n",
      "2.0    7\n",
      "1.0    6\n",
      "7.0    3\n",
      "3.0    1\n",
      "5.0    1\n",
      "6.0    1\n",
      "Name: 10, dtype: int64\n",
      "Fold 8 size: 18\n",
      "2.0    7\n",
      "1.0    6\n",
      "7.0    2\n",
      "3.0    1\n",
      "5.0    1\n",
      "6.0    1\n",
      "Name: 10, dtype: int64\n",
      "Fold 9 size: 17\n",
      "2.0    7\n",
      "1.0    6\n",
      "7.0    2\n",
      "3.0    1\n",
      "5.0    1\n",
      "Name: 10, dtype: int64\n",
      "Fold 10 size: 16\n",
      "1.0    6\n",
      "2.0    6\n",
      "7.0    2\n",
      "3.0    1\n",
      "5.0    1\n",
      "Name: 10, dtype: int64\n",
      "\n",
      "\n",
      "\n",
      " Spliting the data into tunning and remaining\n",
      "Length of tuning data: 18\n",
      "Length of remaining data: 191\n",
      "Length of original data: 209\n",
      "Fold Size 19\n",
      "Max Fold Size:  20\n",
      "KNN- standard\n",
      "\n",
      " Tunning Process Result\n",
      "Best k: 12 with MAE: 0.03186222299732451\n",
      "\n",
      "\n",
      "K : 12\n",
      "\n",
      " Printing the results of one data point\n",
      "Indexes of distnces:  [19  3 24 25 40 76 12  9 18 42 61  5]\n",
      "Values of nearest neighbours:  [0.00437063 0.00437063 0.01048951 0.01398601 0.01223776 0.0270979\n",
      " 0.00087413 0.00611888 0.00874126 0.02272727 0.02272727 0.00524476]\n",
      "Weighted average of these values, which is the Prediction :  0.0043706294494987455\n",
      "\n",
      "\n",
      "Fold: 0\n",
      "Fold: 0, Mean Absolute Error: 0.021931855236135212\n",
      "Fold: 1\n",
      "Fold: 1, Mean Absolute Error: 0.014265789834096581\n",
      "Fold: 2\n",
      "Fold: 2, Mean Absolute Error: 0.01815893178565802\n",
      "Fold: 3\n",
      "Fold: 3, Mean Absolute Error: 0.015168014756375888\n",
      "Fold: 4\n",
      "Fold: 4, Mean Absolute Error: 0.01655871712860518\n",
      "Fold: 5\n",
      "Fold: 5, Mean Absolute Error: 0.013963896370141145\n",
      "Fold: 6\n",
      "Fold: 6, Mean Absolute Error: 0.019511610106673523\n",
      "Fold: 7\n",
      "Fold: 7, Mean Absolute Error: 0.046585666739504566\n",
      "Fold: 8\n",
      "Fold: 8, Mean Absolute Error: 0.07457553008338273\n",
      "Fold: 9\n",
      "Fold: 9, Mean Absolute Error: 0.3097458766545037\n",
      "KNN Standard: Average Mean Absolute Error over all Folds: 0.05504658886950766\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eddited KNN\n",
      " \n",
      "Tunning Part for Eddited KNN \n",
      "Best k: 2, Best Error Threshold: 0.2, Performance: 0.03458624708624709\n",
      "\n",
      " Eddited KNN Main Fuction\n",
      "\n",
      " One Example for edditing error removal\n",
      "Actual value:  0.003496503496503497\n",
      "Predicted value:  0.014423076923076924\n",
      "Error: 0.010926573426573428\n",
      "Error is greater than threshold, so removing this value from dataset\n",
      "\n",
      "\n",
      "Fold: 0\n",
      "Fold 0, Mean Absolute Error: 0.012628818549871182\n",
      "Fold: 1\n",
      "Fold 1, Mean Absolute Error: 0.012812845049687153\n",
      "Fold: 2\n",
      "Fold 2, Mean Absolute Error: 0.0152972027972028\n",
      "Fold: 3\n",
      "Fold 3, Mean Absolute Error: 0.015504232609495768\n",
      "Fold: 4\n",
      "Fold 4, Mean Absolute Error: 0.011179609863820389\n",
      "Fold: 5\n",
      "Fold 5, Mean Absolute Error: 0.013479941111520058\n",
      "Fold: 6\n",
      "Fold 6, Mean Absolute Error: 0.02369341185130659\n",
      "Fold: 7\n",
      "Fold 7, Mean Absolute Error: 0.036805299963194704\n",
      "Fold: 8\n",
      "Fold 8, Mean Absolute Error: 0.06355815237394186\n",
      "Fold: 9\n",
      "Fold 9, Mean Absolute Error: 0.26844405594405585\n",
      "KNN Eddited: Average MAE across all folds: 0.04734035701140964\n",
      "Kmean Clustering\n",
      "K: 5\n",
      "Fold: 0\n",
      "Fold: 0, Mean Absolute Error: 0.9941111520058887\n",
      "Fold: 1\n",
      "Fold: 1, Mean Absolute Error: 0.9873021715126977\n",
      "Fold: 2\n",
      "Fold: 2, Mean Absolute Error: 0.9809992638940007\n",
      "Fold: 3\n",
      "Fold: 3, Mean Absolute Error: 0.9756164887743836\n",
      "Fold: 4\n",
      "Fold: 4, Mean Absolute Error: 0.9675193227824805\n",
      "Fold: 5\n",
      "Fold: 5, Mean Absolute Error: 0.9584100110415898\n",
      "Fold: 6\n",
      "Fold: 6, Mean Absolute Error: 0.9441019506808981\n",
      "Fold: 7\n",
      "Fold: 7, Mean Absolute Error: 0.9066525579683473\n",
      "Fold: 8\n",
      "Fold: 8, Mean Absolute Error: 0.8429333824070665\n",
      "Fold: 9\n",
      "Fold: 9, Mean Absolute Error: 0.5897290209790209\n",
      " KNN Clustering: Average Mean Absolute Error over all Folds: 0.9147375322046374\n",
      "K: 5\n",
      "Fold: 0\n",
      "Fold: 0, Mean Absolute Error: 0.2627738812810139\n",
      "Fold: 1\n",
      "Fold: 1, Mean Absolute Error: 0.25877793346717093\n",
      "Fold: 2\n",
      "Fold: 2, Mean Absolute Error: 0.21339497625633472\n",
      "Fold: 3\n",
      "Fold: 3, Mean Absolute Error: 0.21016267902971633\n",
      "Fold: 4\n",
      "Fold: 4, Mean Absolute Error: 0.2013243038555367\n",
      "Fold: 5\n",
      "Fold: 5, Mean Absolute Error: 0.1903363780212018\n",
      "Fold: 6\n",
      "Fold: 6, Mean Absolute Error: 0.17517674096235905\n",
      "Fold: 7\n",
      "Fold: 7, Mean Absolute Error: 0.12513181799988546\n",
      "Fold: 8\n",
      "Fold: 8, Mean Absolute Error: 0.06067744373339639\n",
      "Fold: 9\n",
      "Fold: 9, Mean Absolute Error: 0.21861498203586302\n",
      " KNN Clustering: Average Mean Absolute Error over all Folds: 0.1916371136642478\n",
      "K: 5\n",
      "Fold: 0\n",
      "Fold: 0, Mean Absolute Error: 1.9941111520058887\n",
      "Fold: 1\n",
      "Fold: 1, Mean Absolute Error: 1.987302171512698\n",
      "Fold: 2\n",
      "Fold: 2, Mean Absolute Error: 1.9809992638940006\n",
      "Fold: 3\n",
      "Fold: 3, Mean Absolute Error: 1.975616488774384\n",
      "Fold: 4\n",
      "Fold: 4, Mean Absolute Error: 1.9675193227824803\n",
      "Fold: 5\n",
      "Fold: 5, Mean Absolute Error: 1.9584100110415903\n",
      "Fold: 6\n",
      "Fold: 6, Mean Absolute Error: 1.9441019506808979\n",
      "Fold: 7\n",
      "Fold: 7, Mean Absolute Error: 1.9066525579683478\n",
      "Fold: 8\n",
      "Fold: 8, Mean Absolute Error: 1.8429333824070666\n",
      "Fold: 9\n",
      "Fold: 9, Mean Absolute Error: 1.5897290209790211\n",
      " KNN Clustering: Average Mean Absolute Error over all Folds: 1.9147375322046376\n",
      "K: 5\n",
      "Fold: 0\n",
      "Fold: 0, Mean Absolute Error: 0.9941111520058887\n",
      "Fold: 1\n",
      "Fold: 1, Mean Absolute Error: 0.987302171512698\n",
      "Fold: 2\n",
      "Fold: 2, Mean Absolute Error: 0.9809992638940007\n",
      "Fold: 3\n",
      "Fold: 3, Mean Absolute Error: 0.9756164887743836\n",
      "Fold: 4\n",
      "Fold: 4, Mean Absolute Error: 0.9675193227824807\n",
      "Fold: 5\n",
      "Fold: 5, Mean Absolute Error: 0.9584100110415898\n",
      "Fold: 6\n",
      "Fold: 6, Mean Absolute Error: 0.9441019506808981\n",
      "Fold: 7\n",
      "Fold: 7, Mean Absolute Error: 0.9066525579683473\n",
      "Fold: 8\n",
      "Fold: 8, Mean Absolute Error: 0.8429333824070667\n",
      "Fold: 9\n",
      "Fold: 9, Mean Absolute Error: 0.5897290209790209\n",
      " KNN Clustering: Average Mean Absolute Error over all Folds: 0.9147375322046376\n",
      "K: 5\n",
      "Fold: 0\n",
      "Fold: 0, Mean Absolute Error: 3.489662532334677\n",
      "Fold: 1\n",
      "Fold: 1, Mean Absolute Error: 3.637731746581659\n",
      "Fold: 2\n",
      "Fold: 2, Mean Absolute Error: 3.866449333407653\n",
      "Fold: 3\n",
      "Fold: 3, Mean Absolute Error: 3.8929502104546856\n",
      "Fold: 4\n",
      "Fold: 4, Mean Absolute Error: 3.511951822227399\n",
      "Fold: 5\n",
      "Fold: 5, Mean Absolute Error: 3.5505837290797713\n",
      "Fold: 6\n",
      "Fold: 6, Mean Absolute Error: 3.697014638239431\n",
      "Fold: 7\n",
      "Fold: 7, Mean Absolute Error: 3.4162367144119217\n",
      "Fold: 8\n",
      "Fold: 8, Mean Absolute Error: 3.680574679861759\n",
      "Fold: 9\n",
      "Fold: 9, Mean Absolute Error: 3.8516566457676973\n",
      " KNN Clustering: Average Mean Absolute Error over all Folds: 3.6594812052366654\n",
      "K: 5\n",
      "Fold: 0\n",
      "Fold: 0, Mean Absolute Error: 4.704416122639673\n",
      "Fold: 1\n",
      "Fold: 1, Mean Absolute Error: 4.745963049924693\n",
      "Fold: 2\n",
      "Fold: 2, Mean Absolute Error: 3.809969809241642\n",
      "Fold: 3\n",
      "Fold: 3, Mean Absolute Error: 3.8539360896357993\n",
      "Fold: 4\n",
      "Fold: 4, Mean Absolute Error: 4.4314446448836815\n",
      "Fold: 5\n",
      "Fold: 5, Mean Absolute Error: 4.023268389616464\n",
      "Fold: 6\n",
      "Fold: 6, Mean Absolute Error: 4.031607384376998\n",
      "Fold: 7\n",
      "Fold: 7, Mean Absolute Error: 4.42739625695283\n",
      "Fold: 8\n",
      "Fold: 8, Mean Absolute Error: 4.39436878329491\n",
      "Fold: 9\n",
      "Fold: 9, Mean Absolute Error: 5.54365321905629\n",
      " KNN Clustering: Average Mean Absolute Error over all Folds: 4.396602374962297\n",
      "\n",
      "\n",
      "Best kc: 3, with MAE: 0.1916371136642478\n",
      "K : 3\n",
      "Cluster Sizes: {0: 127, 1: 19, 2: 25}\n",
      "Tuning with bandwidth: 0.1\n",
      "Bandwidth: 0.1, Average MAE: 1.596007989918973\n",
      "Tuning with bandwidth: 0.5\n",
      "Bandwidth: 0.5, Average MAE: 1.1508427372157903\n",
      "Tuning with bandwidth: 1\n",
      "Bandwidth: 1, Average MAE: 0.9748857192299634\n",
      "Tuning with bandwidth: 2\n",
      "Bandwidth: 2, Average MAE: 0.9275302266260107\n",
      "Tuning with bandwidth: 5\n",
      "Bandwidth: 5, Average MAE: 0.9142038359978735\n",
      "Tuning with bandwidth: 10\n",
      "Bandwidth: 10, Average MAE: 0.9122995060132044\n",
      "Best bandwidth: 10 with MAE: 0.9122995060132044\n",
      "K: 2\n",
      "Fold: 0\n",
      "Fold: 0, Mean Absolute Error: 0.9945630956123304\n",
      "Fold: 1\n",
      "Fold: 1, Mean Absolute Error: 0.9875370509993264\n",
      "Fold: 2\n",
      "Fold: 2, Mean Absolute Error: 0.981703734454125\n",
      "Fold: 3\n",
      "Fold: 3, Mean Absolute Error: 0.9764404509699471\n",
      "Fold: 4\n",
      "Fold: 4, Mean Absolute Error: 0.9678434023830425\n",
      "Fold: 5\n",
      "Fold: 5, Mean Absolute Error: 0.9586032049264811\n",
      "Fold: 6\n",
      "Fold: 6, Mean Absolute Error: 0.9444668368536632\n",
      "Fold: 7\n",
      "Fold: 7, Mean Absolute Error: 0.9070086669400398\n",
      "Fold: 8\n",
      "Fold: 8, Mean Absolute Error: 0.8433023207227098\n",
      "Fold: 9\n",
      "Fold: 9, Mean Absolute Error: 0.5901515503046143\n",
      " KNN Clustering: Average Mean Absolute Error over all Folds: 0.915162031416628\n"
     ]
    }
   ],
   "source": [
    "#Main driver class calls all other classes  \n",
    "\n",
    "class Driver:\n",
    "    def __init__(self):\n",
    "        self.DSNames_classification = 'glass', 'soybean-small', 'breast-cancer-wisconsin'\n",
    "        self.DSNames_regression = 'abalone', 'machine', 'forestfires'\n",
    "    \n",
    "    def main_classification(self):\n",
    "        #change the dataset number here from 0 to 2\n",
    "        ds = DataSet(self.DSNames_classification[0])\n",
    "        ds.type = 'Classification'\n",
    "        ds_reader = Dataset_Reader()\n",
    "        ds_reader.read_data_file(ds)\n",
    "            \n",
    "        pp = PreProcess(ds)\n",
    "        pp.split_tuning_data_classification(ds)\n",
    "        knn = KNN_Classification()\n",
    "        pp.split_data_classification(ds)\n",
    "       \n",
    "        \n",
    "    #Regression\n",
    "    def main_regression(self):\n",
    "        #change the dataset number here from 0 to 2\n",
    "        ds = DataSet(self.DSNames_regression[1])\n",
    "        ds.type = 'Regression'\n",
    "        ds_reader = Dataset_Reader()\n",
    "        ds_reader.read_data_file(ds)\n",
    "        \n",
    "        pp = PreProcess(ds)\n",
    "        pp.split_tuning_data_regression(ds)\n",
    "        pp.split_data_regression(ds)\n",
    "        \n",
    "        print('KNN- standard')\n",
    "        knn = KNN_Regression()\n",
    "        knn.tuning_k(ds)\n",
    "        knn.mainFunction(ds)\n",
    "        \n",
    "        print('Eddited KNN')\n",
    "        ke = EditedKNNRegression()\n",
    "        ke.tune_k_and_threshold(ds)\n",
    "        ke.mainFunction(ds)\n",
    "        \n",
    "        print('Kmean Clustering')\n",
    "        #pass the k from edited knn to clusters\n",
    "        #this class will make clusters\n",
    "        kmc = KMeansClustering()\n",
    "        kmc.tuning_kc(ds)\n",
    "        #kmc = KMeansClustering(k=3)\n",
    "        kmc.mainFunction(ds)\n",
    "        #pass the centroids and values to this class\n",
    "        kgr = KNN_Gaussian_Regression(centroids = kmc.centroids, centroid_values= kmc.labels, k =ke.k)\n",
    "        kgr.tuning_bandwidth(ds)\n",
    "        kgr.mainFunction(ds)\n",
    "        \n",
    "        \n",
    "#calling the driver class here.    \n",
    "dr = Driver()\n",
    "print('\\nTen Fold Demonstration')\n",
    "dr.main_classification()\n",
    "print('\\n')\n",
    "dr.main_regression()    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f529548",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forest Fire DataSet\n",
    "#sample1: is the result of K-mean Clustering\n",
    "sample01 = [1.693601485800591e-05, 1.9925523473382564e-05, 1.753379018276312e-05,\n",
    "           3.3876317633102746e-06, 2.072171857836256e-05, 0.0010627057078018106,\n",
    "           0.002668567316247554, 0.005658917432272502, 0.01183330488038679,0.07339555164437979\n",
    "          ]\n",
    "        \n",
    "#sample2: is the result of Eddited KNN\n",
    "sample02 = [0.005490166850875838, 0.006520172466240352, 0.00580724440121342, \n",
    "           0.006296688009455009, 0.006613248268581466, 0.006337277975510517,\n",
    "           0.003992872412519772, 0.00524353277293915, 0.007642246013238997, 0.07076117742044265\n",
    "          ]\n",
    "#sample 3 is the result of standard KNN\n",
    "sample03 = [0.013927713588399985, 0.013628125542312597, 0.008772664483666273,\n",
    "           0.01215639936156414, 0.009550144921636047, 0.013768467315543821, \n",
    "           0.009013802391241747,0.005546416670753066, 0.009929662903456302,0.07068575698088851\n",
    "          ]\n",
    "#Machine Dataset\n",
    "#sample4: is the result of K-mean Clustering\n",
    "sample4 = [0.005888847994111152, 0.012697828487302174, 0.01900073610599926,\n",
    "           0.02438351122561649, 0.03248067721751933, 0.041589988958410005,\n",
    "           0.05589804931910195, 0.09334744203165256, 0.1570666175929334,0.410270979020979]\n",
    "        \n",
    "#sample5: is the result of Eddited KNN\n",
    "sample5 = [0.012628818549871182, 0.012812845049687153, 0.0152972027972028, \n",
    "           0.015504232609495768, 0.011179609863820389, 0.013479941111520058,\n",
    "           0.02369341185130659, 0.036805299963194704, 0.06355815237394186, 0.26844405594405585\n",
    "          ]\n",
    "#sample6 is the result of standard KNN\n",
    "sample6 = [0.021931855236135212, 0.014265789834096581, 0.01815893178565802,\n",
    "           0.015168014756375888, 0.01655871712860518, 0.013963896370141145, \n",
    "           0.019511610106673523,0.046585666739504566, 0.07457553008338273,0.3097458766545037\n",
    "          ]\n",
    "\n",
    "#Abalone Dataset\n",
    "#sample7: is the result of K-mean Clustering\n",
    "sample7 = [0.3424158838481524, 0.2887105735386065, 0.2503090781734497,\n",
    "           0.2283028365757166, 0.21542446064982448, 0.18100929700661064,\n",
    "           0.16394339537095778, 0.13429465683713254, 0.07687529605393084,0.0786549492199398\n",
    "          ]\n",
    "        \n",
    "#sample8: is the result of Eddited KNN\n",
    "sample8 = [0.058263677811550174, 0.04523176291793312, 0.04416793313069908, \n",
    "           0.04428191489361702, 0.04148936170212766, 0.040463525835866265,\n",
    "           0.04141337386018237, 0.051462765957446824, 0.09057750759878419, 0.2096362954630671\n",
    "          ]\n",
    "#sample9 is the result of standard KNN\n",
    "sample9 = [0.06352171750527556, 0.04673897251447117, 0.049430548728637405,\n",
    "           0.050182907975997344, 0.046138741194388204, 0.0446938496523912, \n",
    "           0.04151049556938836,0.046943298498270794, 0.07354661352087384,0.2097285526418535\n",
    "          ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5391812c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now once we have results, we will apply statistcal tests on it.\n",
    "#to prove if the hypothesis\n",
    "\n",
    "def statistical_test():\n",
    "        import scipy.stats as stats  # Import necessary statistical module\n",
    "        \n",
    "        # Perform the Shapiro-Wilk test for normality on both sets of samples\n",
    "        shapiro_statistic, p_value = stats.shapiro(sample1)\n",
    "        shapiro_statistic, p_value2 = stats.shapiro(sample2)\n",
    "\n",
    "        # Output the results of the Shapiro-Wilk test\n",
    "        print(f\"Shapiro-Wilk Test Statistic: {shapiro_statistic}\")\n",
    "        print(f\"P-value: {p_value}\")\n",
    "\n",
    "        # If both samples are normally distributed (p > 0.05), proceed with a paired t-test\n",
    "        if p_value > 0.05 and p_value2 > 0.05:\n",
    "            print(\"The differences are normally distributed.\")\n",
    "\n",
    "            # Perform a paired t-test to compare the two sets of samples\n",
    "            t_statistic, t_p_value = stats.ttest_rel(sample1, sample2)\n",
    "\n",
    "            # Output the result of the t-test\n",
    "            print(f'Paired t-test p-value: {t_p_value}')\n",
    "\n",
    "            # Check if the p-value is below 0.05, indicating a statistically significant difference\n",
    "            if t_p_value < 0.05:\n",
    "                print(\"The two models are significantly different.\")\n",
    "            else:\n",
    "                print(\"The two models are not significantly different.\")\n",
    "        else:\n",
    "            # If the samples are not normally distributed, perform a non-parametric Wilcoxon signed-rank test\n",
    "            print(\"The differences are not normally distributed.\")\n",
    "\n",
    "            # Perform the Wilcoxon signed-rank test to compare the two sets of samples\n",
    "            wilcoxon_statistic, wilcoxon_p_value = stats.wilcoxon(sample1, sample2)\n",
    "\n",
    "            # Output the result of the Wilcoxon test\n",
    "            print(f'Wilcoxon signed-rank test p-value: {wilcoxon_p_value}')\n",
    "            print(f'Wilcoxon Test statistic: {wilcoxon_statistic}, P-value: {wilcoxon_p_value}')\n",
    "\n",
    "            # Check if the p-value is below 0.05, indicating a statistically significant difference\n",
    "            if wilcoxon_p_value < 0.05:\n",
    "                print(\"The two models are significantly different.\")\n",
    "            else:\n",
    "                print(\"The two models are not significantly different.\")\n",
    "              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b8e44a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Shapiro-Wilk Test Statistic: 0.410786509513855\n",
      "P-value: 2.394991497567389e-06\n",
      "The differences are not normally distributed.\n",
      "Wilcoxon signed-rank test p-value: 0.048828125\n",
      "Wilcoxon Test statistic: 8.0, P-value: 0.048828125\n",
      "The two models are significantly different.\n",
      "2\n",
      "Shapiro-Wilk Test Statistic: 0.5141381025314331\n",
      "P-value: 0.00028908593230880797\n",
      "The differences are not normally distributed.\n",
      "Wilcoxon signed-rank test p-value: 0.013671875\n",
      "Wilcoxon Test statistic: 4.0, P-value: 0.013671875\n",
      "The two models are significantly different.\n",
      "4\n",
      "Shapiro-Wilk Test Statistic: 0.5605525970458984\n",
      "P-value: 0.8808746933937073\n",
      "The differences are not normally distributed.\n",
      "Wilcoxon signed-rank test p-value: 0.01953125\n",
      "Wilcoxon Test statistic: 5.0, P-value: 0.01953125\n",
      "The two models are significantly different.\n"
     ]
    }
   ],
   "source": [
    "#this is to test the difference of eddited and k mean clustering results\n",
    "#for each dataset\n",
    "\n",
    "list_samples = [sample01, sample02, sample4, sample5, sample7, sample8 ]\n",
    "i = 0\n",
    "while i < len(list_samples):  \n",
    "    print(i)\n",
    "    sample1 = list_samples[i]  \n",
    "    sample2 = list_samples[i + 1]  \n",
    "    \n",
    "    i += 2\n",
    "    # Call the statistical test with the two consecutive samples\n",
    "    statistical_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51d77c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Shapiro-Wilk Test Statistic: 0.410786509513855\n",
      "P-value: 3.712407988132327e-06\n",
      "The differences are not normally distributed.\n",
      "Wilcoxon signed-rank test p-value: 0.00390625\n",
      "Wilcoxon Test statistic: 1.0, P-value: 0.00390625\n",
      "The two models are significantly different.\n",
      "2\n",
      "Shapiro-Wilk Test Statistic: 0.5141381025314331\n",
      "P-value: 5.332392447598977e-06\n",
      "The differences are not normally distributed.\n",
      "Wilcoxon signed-rank test p-value: 0.02734375\n",
      "Wilcoxon Test statistic: 6.0, P-value: 0.02734375\n",
      "The two models are significantly different.\n",
      "4\n",
      "Shapiro-Wilk Test Statistic: 0.5605525970458984\n",
      "P-value: 6.105638476583408e-06\n",
      "The differences are not normally distributed.\n",
      "Wilcoxon signed-rank test p-value: 0.232421875\n",
      "Wilcoxon Test statistic: 15.0, P-value: 0.232421875\n",
      "The two models are not significantly different.\n"
     ]
    }
   ],
   "source": [
    "#Next we will test the standard with eddited  \n",
    "\n",
    "list_samples = [sample03, sample02, sample6, sample5, sample9, sample8 ]\n",
    "i = 0\n",
    "while i < len(list_samples):  \n",
    "    print(i)\n",
    "    sample1 = list_samples[i]  \n",
    "    sample2 = list_samples[i + 1]  \n",
    "    \n",
    "    i += 2\n",
    "    # Call the statistical test with the two consecutive samples\n",
    "    statistical_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b24edb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Shapiro-Wilk Test Statistic: 0.4845079779624939\n",
      "P-value: 3.712407988132327e-06\n",
      "The differences are not normally distributed.\n",
      "Wilcoxon signed-rank test p-value: 0.02734375\n",
      "Wilcoxon Test statistic: 6.0, P-value: 0.02734375\n",
      "The two models are significantly different.\n",
      "2\n",
      "Shapiro-Wilk Test Statistic: 0.6599441766738892\n",
      "P-value: 5.332392447598977e-06\n",
      "The differences are not normally distributed.\n",
      "Wilcoxon signed-rank test p-value: 0.037109375\n",
      "Wilcoxon Test statistic: 7.0, P-value: 0.037109375\n",
      "The two models are significantly different.\n",
      "4\n",
      "Shapiro-Wilk Test Statistic: 0.9689428806304932\n",
      "P-value: 6.105638476583408e-06\n",
      "The differences are not normally distributed.\n",
      "Wilcoxon signed-rank test p-value: 0.013671875\n",
      "Wilcoxon Test statistic: 4.0, P-value: 0.013671875\n",
      "The two models are significantly different.\n"
     ]
    }
   ],
   "source": [
    "# This will test the standard with k mean clustering  \n",
    "\n",
    "list_samples = [sample03, sample01, sample6, sample4, sample9, sample7 ]\n",
    "i = 0\n",
    "while i < len(list_samples):  \n",
    "    print(i)\n",
    "    sample1 = list_samples[i]  \n",
    "    sample2 = list_samples[i + 1]  \n",
    "    \n",
    "    i += 2\n",
    "    # Call the statistical test with the two consecutive samples\n",
    "    statistical_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21e0b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
